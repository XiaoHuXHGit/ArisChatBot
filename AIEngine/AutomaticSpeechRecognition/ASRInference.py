import os

import pyaudio
import torch
import librosa
from typing import Optional, Generator

from numpy import ndarray
import numpy as np
from transformers.modeling_utils import SpecificPreTrainedModelType
from transformers.processing_utils import SpecificProcessorType
from silero_vad import load_silero_vad, get_speech_timestamps, read_audio

from configs import ConfigManager


class ASRInferToolTorch:
    def __init__(
            self,
            # processor: SpecificProcessorType,
            # models: SpecificPreTrainedModelType,
            model_path: Optional[str],
            device: Optional[str] = "cpu",
            data_type: Optional[torch.dtype] = torch.float32,
    ):
        # AI models settings
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model not found: {model_path}")
        self.model_path = model_path
        self.device = device
        self.data_type = data_type
        self.processor = None
        self.model = None
        self.model_load()

    def inference(self, audio: ndarray, language: Optional[str] = None, task: str = "transcribe") -> str:
        """
        è½¬å½•éŸ³é¢‘ã€‚

        Args:
            audio: éŸ³é¢‘æ–‡ä»¶äºŒè¿›åˆ¶æ•°æ®
            language: è¯­è¨€ä»£ç ï¼ˆå¦‚ "chinese", "english"ï¼‰ï¼ŒNone è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
            task: "transcribe"ï¼ˆè½¬å½•ï¼‰æˆ– "translate"ï¼ˆç¿»è¯‘æˆè‹±æ–‡ï¼‰
        """
        if self.model is None or self.processor is None:
            self.model_load()
        input_features = self.processor(
            audio,
            sampling_rate=16000,
            return_tensors="pt"
        ).input_features.to(self.device, dtype=self.data_type)

        # è®¾ç½®ç”Ÿæˆå‚æ•°
        with torch.no_grad():
            predicted_ids = self.model.generate(
                input_features,
                task=task,
                language=language,
                attention_mask=torch.ones((1, input_features.shape[-1] // 2), device=self.device)
            )
        transcription = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
        return transcription

    def model_clear(self) -> None:
        """
        æ¸…é™¤æ¨¡å‹çš„æ˜¾å­˜ã€‚
        """
        self.model: SpecificPreTrainedModelType | None = None
        self.processor: SpecificProcessorType | None = None
        if "cuda" in self.device:
            torch.cuda.empty_cache()

    def model_load(self) -> None:
        from transformers import WhisperProcessor, WhisperForConditionalGeneration
        self.processor = WhisperProcessor.from_pretrained(self.model_path)
        self.model = WhisperForConditionalGeneration.from_pretrained(
            self.model_path,
            dtype=self.data_type,
            low_cpu_mem_usage=True,
            use_safetensors=True
        ).to(self.device).eval()

    def model_offload_to_cpu(self, device: str = "cpu") -> None:
        self.model = self.model.to(device)
        self.processor = self.processor.to(device)


class ASRInferToolNPU:
    def __init__(self, model_path: Optional[str] = None):
        pass

    def inference(self, audio_path: str, language: Optional[str] = None, task: str = "transcribe") -> str:
        pass

    def realtime_inference(self):
        pass


class ASRInference:
    """
    Unified ASR inference interface.
    Automatically selects backend based on device.
    ==================== ä¸­æ–‡è¯´æ˜ ====================
    ç»Ÿä¸€çš„ ASR æ¨ç†æ¥å£ã€‚
    è‡ªåŠ¨é€‰æ‹©åç«¯ï¼Œæ ¹æ®è®¾å¤‡ã€‚
    """

    def __init__(
            self,
            model_path: Optional[str],
            device: Optional[str] = "auto",
            vad_model_path: Optional[str] = None
    ):
        if device == "auto":
            # from openvino import Core
            # core = Core()
            # if "NPU" in core.available_devices:
            #     device = "npu"
            if torch.cuda.is_available():
                device = "cuda"
            else:
                device = "cpu"
            # del core

        self.model_path = model_path
        self.device = device
        self.data_type = torch.float16 if self.device != "cpu" else torch.float32

        if device == "cuda" or device == "cpu":
            self.backend = ASRInferToolTorch(model_path, device=device, data_type=self.data_type)
        # elif device == "npu":
        #     self.backend = ASRInferToolNPU(model_path=model_path)
        else:
            raise ValueError(f"Unsupported device: {device}")

        # microphone settings
        # === VAD åˆå§‹åŒ–ï¼ˆä½¿ç”¨ silero-vadï¼‰===
        self.sample_rate = 16000
        if vad_model_path == "default":
            self.vad_model = load_silero_vad()
        else:
            if not vad_model_path:
                vad_model_path = os.path.join(os.path.dirname(__file__), "models", "vad_model")
                if not os.path.exists(vad_model_path):
                    os.makedirs(vad_model_path, exist_ok=True)
            self.vad_model = torch.package.PackageImporter(vad_model_path).load_pickle("silero_vad", "models")
        self.vad_device = self.device
        self.vad_model.to(self.vad_device)

        # VAD å‚æ•°ï¼ˆå¯è°ƒï¼‰
        self.speech_pad_ms = 300  # è¯­éŸ³å‰åå¡«å……ï¼ˆæ¯«ç§’ï¼‰
        self.min_speech_duration_ms = 300
        self.max_speech_duration_s = 10.0
        self.min_silence_duration_ms = 1000  # åˆ¤å®šè¯­éŸ³ç»“æŸçš„é™éŸ³æ—¶é•¿

    def model_load(self):
        self.backend.model_load()

    def model_clear(self):
        self.backend.model_clear()

    def local_file_inference(self, audio_path: str, language: Optional[str] = None, task: str = "transcribe") -> str:
        """
        Transcribe audio file to text.
        :param audio_path: Path to audio file (supports WAV, MP3, etc. via librosa)
        :param language:
        :param task:
        :return: Transcribed text
        """
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"Audio file not found: {audio_path}")
        audio, sr = librosa.load(audio_path, sr=16000, mono=True)
        return self.backend.inference(audio, language=language, task=task)

    def _int16_to_float32(self, audio_int16: np.ndarray) -> np.ndarray:
        return audio_int16.astype(np.float32) / 32768.0

    def microphone_audio_stream(self) -> Generator[np.ndarray, None, None]:
        """
        ä½¿ç”¨ silero-vad å®ç°æ›´ç²¾å‡†çš„è¯­éŸ³æ´»åŠ¨æ£€æµ‹ã€‚
        Yield: æœ‰æ•ˆè¯­éŸ³ç‰‡æ®µï¼ˆ16kHz, float32, monoï¼‰
        """
        p = pyaudio.PyAudio()
        # æ³¨æ„ï¼šsilero-vad æ¨è 16kHzï¼Œæˆ‘ä»¬ç›´æ¥é‡‡ 16kHz
        stream = p.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=4800  # 300ms chunkï¼ˆå¯è°ƒï¼‰
        )

        audio_buffer = np.array([], dtype=np.float32)
        chunk_duration_sec = 0.3  # 300ms

        try:
            while True:
                # è¯»å–ä¸€å—éŸ³é¢‘
                chunk_bytes = stream.read(int(self.sample_rate * chunk_duration_sec), exception_on_overflow=False)
                chunk_int16 = np.frombuffer(chunk_bytes, dtype=np.int16)
                chunk_float32 = self._int16_to_float32(chunk_int16)

                # ç´¯ç§¯åˆ°ç¼“å†²åŒº
                audio_buffer = np.concatenate([audio_buffer, chunk_float32])

                # å¦‚æœç¼“å†²åŒºå¤ªé•¿ï¼ˆæ¯”å¦‚ > 15 ç§’ï¼‰ï¼Œå¼ºåˆ¶æˆªæ–­é¿å…å†…å­˜çˆ†ç‚¸
                if len(audio_buffer) > self.sample_rate * 15:
                    audio_buffer = audio_buffer[-int(self.sample_rate * 10):]

                # è½¬ä¸º torch tensor å¹¶ç§»åˆ°è®¾å¤‡
                wav = torch.from_numpy(audio_buffer).to(self.vad_device)

                # è¿è¡Œ VAD æ£€æµ‹
                speech_timestamps = get_speech_timestamps(
                    wav,
                    self.vad_model,
                    sampling_rate=self.sample_rate,
                    min_speech_duration_ms=self.min_speech_duration_ms,
                    min_silence_duration_ms=self.min_silence_duration_ms,
                    speech_pad_ms=self.speech_pad_ms,
                    return_seconds=False
                )

                # å¦‚æœæ£€æµ‹åˆ°å®Œæ•´è¯­éŸ³æ®µï¼ˆä¸”æ˜¯æ–°æ®µï¼‰
                if speech_timestamps:
                    # å–æœ€åä¸€ä¸ªå®Œæ•´è¯­éŸ³æ®µï¼ˆå‡è®¾ç”¨æˆ·æ­£åœ¨è¯´è¯ï¼‰
                    last_seg = speech_timestamps[-1]
                    start, end = last_seg['start'], last_seg['end']

                    # é¿å…é‡å¤è¾“å‡ºåŒä¸€æ®µ
                    if end <= len(audio_buffer):
                        speech_segment = audio_buffer[start:end].copy()
                        yield speech_segment

                        # æ¸…ç©ºå·²å¤„ç†éƒ¨åˆ†ï¼ˆä¿ç•™ä¸€ç‚¹å°¾éƒ¨ä»¥é˜²åˆ‡å‰²ï¼‰
                        audio_buffer = audio_buffer[end - int(self.sample_rate * 0.1):]

        except KeyboardInterrupt:
            pass
        finally:
            stream.stop_stream()
            stream.close()
            p.terminate()

    def microphone_inference(self, language: Optional[str] = None, task: str = "transcribe") -> Generator[str, None, None]:
        """
        å®æ—¶éº¦å…‹é£è½¬å½•ï¼šé‡‡é›† â†’ VAD â†’ æ¨ç† â†’ yield æ–‡æœ¬
        """
        if not hasattr(self.backend, 'inference'):
            raise NotImplementedError("Backend does not support raw audio transcription.")

        for audio_segment in self.microphone_audio_stream():
            text = self.backend.inference(audio_segment, language=language, task=task)
            yield text.strip()


# Example usage (uncomment for testing)
if __name__ == "__main__":
    config = ConfigManager()
    asr = ASRInference(model_path=config.asr_config.model_path, device=config.asr_config.device,
                       vad_model_path=config.asr_config.vad_model_path)
    while True:
        audio_path = input("path: ").strip("\"")
        if audio_path == "exit":
            break
        elif audio_path == "clear":
            asr.model_clear()
            continue
        elif audio_path == "load":
            asr.model_load()
            continue
        import time

        start = time.time()
        text = asr.local_file_inference(audio_path, language="chinese")
        end = time.time()
        print("Time:", end - start)
        print("Transcription:", text)
    del asr
    torch.cuda.empty_cache()

    # asr = ASRInference(model_path=config.asr_config.model_path, device="cuda",
    #                    vad_model_path=config.asr_config.vad_model_path)
    # for text in asr.microphone_inference(language="chinese"):
    #     print("ğŸ—£ï¸:", text)
    # del asr
    # torch.cuda.empty_cache()
