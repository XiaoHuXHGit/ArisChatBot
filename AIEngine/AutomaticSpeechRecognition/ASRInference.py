"""
å­˜åœ¨bugï¼šè¯†åˆ«å‡ºæ¥ä¸­æ–‡æ²¡æ ‡ç‚¹
ä¿®å¤æ–¹æ³•ï¼šå¾®è°ƒä¼˜åŒ–æˆ–è€…ç›´æ¥æ¢æ¨¡å‹
"""
import os
import pyaudio
import torch
import librosa
from typing import Optional, Generator
from numpy import ndarray
import numpy as np
import logging
from transformers.modeling_utils import SpecificPreTrainedModelType
from transformers.processing_utils import SpecificProcessorType
from silero_vad import load_silero_vad, get_speech_timestamps
from configs import ConfigManager


class ASRInferToolTorch:
    def __init__(
            self,
            model_path: Optional[str],
            device: Optional[str] = None,  # ä¿®æ”¹é»˜è®¤å€¼ä¸ºNone
            data_type: Optional[torch.dtype] = torch.float32,
    ):
        if model_path and not os.path.exists(model_path):
            raise FileNotFoundError(f"Model not found: {model_path}")
        self.model_path = model_path
        self.device = device
        self.data_type = data_type
        self.processor = None
        self.model = None

    def inference(self, audio: ndarray, language: Optional[str] = None, task: str = "transcribe") -> str:
        # æ£€æµ‹æ¨¡å‹æ˜¯å¦ä¸ºNoneï¼Œå¦‚æœæ˜¯åˆ™é»˜è®¤åŠ è½½åˆ°CPU
        if self.model is None or self.processor is None:
            logging.info("æœªé€‰æ‹©åˆå§‹åŒ–è®¾å¤‡ï¼Œé»˜è®¤å°†æ¨¡å‹åŠ è½½åˆ°CPU")
            self.load_to_cpu()

        # ä½¿ç”¨æ¨¡å‹å½“å‰çš„æ•°æ®ç±»å‹æ¥å¤„ç†è¾“å…¥ç‰¹å¾
        input_features = self.processor(
            audio,
            sampling_rate=16000,
            return_tensors="pt"
        ).input_features.to(self.device, dtype=self.model.dtype)  # å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨æ¨¡å‹çš„å®é™…æ•°æ®ç±»å‹

        with torch.no_grad():
            predicted_ids = self.model.generate(
                input_features,
                task=task,
                language=language,
                attention_mask=torch.ones((1, input_features.shape[-1] // 2), device=self.device)
            )
        transcription = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
        return transcription

    def model_clear(self) -> None:
        self.model: SpecificPreTrainedModelType | None = None
        self.processor: SpecificProcessorType | None = None
        if "cuda" in self.device:
            torch.cuda.empty_cache()

    def model_load(self) -> None:
        """åŸæœ‰æ–¹æ³•ï¼Œç°åœ¨ç”¨äºä»ç¡¬ç›˜åŠ è½½æ¨¡å‹"""
        from transformers import WhisperProcessor, WhisperForConditionalGeneration
        self.processor = WhisperProcessor.from_pretrained(self.model_path)
        self.model = WhisperForConditionalGeneration.from_pretrained(
            self.model_path,
            dtype=self.data_type,
            low_cpu_mem_usage=True,
            use_safetensors=True
        ).to(self.device).eval()

    def load_to_cpu(self) -> None:
        """å°†æ¨¡å‹åŠ è½½åˆ°CPUï¼Œå¦‚æœæ¨¡å‹æœªåŠ è½½åˆ™ä»ç¡¬ç›˜åŠ è½½ï¼Œå¦åˆ™åœ¨å†…å­˜å’Œæ˜¾å­˜é—´è°ƒåº¦"""
        device = "cpu"
        data_type = torch.float32  # CPUé»˜è®¤ä½¿ç”¨float32

        if self.model is None or self.processor is None:
            # æ¨¡å‹æœªåŠ è½½ï¼Œä»ç¡¬ç›˜åŠ è½½
            from transformers import WhisperProcessor, WhisperForConditionalGeneration
            self.processor = WhisperProcessor.from_pretrained(self.model_path)
            self.model = WhisperForConditionalGeneration.from_pretrained(
                self.model_path,
                dtype=data_type,
                low_cpu_mem_usage=True,
                use_safetensors=True
            ).to(device).eval()
        else:
            # æ¨¡å‹å·²åŠ è½½ï¼Œç›´æ¥ç§»åŠ¨åˆ°CPU
            self.model = self.model.to(device).to(dtype=data_type)  # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            if hasattr(self.processor, 'to'):
                self.processor = self.processor.to(device)

        self.device = device
        self.data_type = data_type

    def load_to_gpu(self, device_num: int = 0, if_half: bool = True) -> None:
        """å°†æ¨¡å‹åŠ è½½åˆ°GPUï¼Œå¦‚æœæ¨¡å‹æœªåŠ è½½åˆ™ä»ç¡¬ç›˜åŠ è½½ï¼Œå¦åˆ™åœ¨å†…å­˜å’Œæ˜¾å­˜é—´è°ƒåº¦"""
        device = f"cuda:{device_num}" if torch.cuda.is_available() else "cuda"
        data_type = torch.float16 if if_half else torch.float32  # GPUå¯é€‰æ‹©æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦

        if self.model is None or self.processor is None:
            # æ¨¡å‹æœªåŠ è½½ï¼Œä»ç¡¬ç›˜åŠ è½½
            from transformers import WhisperProcessor, WhisperForConditionalGeneration
            self.processor = WhisperProcessor.from_pretrained(self.model_path)
            self.model = WhisperForConditionalGeneration.from_pretrained(
                self.model_path,
                dtype=data_type,
                low_cpu_mem_usage=True,
                use_safetensors=True
            ).to(device).eval()
        else:
            # æ¨¡å‹å·²åŠ è½½ï¼Œç›´æ¥ç§»åŠ¨åˆ°GPUå¹¶è½¬æ¢æ•°æ®ç±»å‹
            self.model = self.model.to(device).to(dtype=data_type)  # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            if hasattr(self.processor, 'to'):
                self.processor = self.processor.to(device)

        self.device = device
        self.data_type = data_type


class ASRInferToolNPU:
    def __init__(self, model_path: Optional[str] = None):
        pass

    def inference(self, audio_path: str, language: Optional[str] = None, task: str = "transcribe") -> str:
        pass

    def realtime_inference(self):
        pass


class ASRInference:
    """
    Unified ASR inference interface.
    Automatically selects backend based on device.
    ==================== ä¸­æ–‡è¯´æ˜ ====================
    ç»Ÿä¸€çš„ ASR æ¨ç†æ¥å£ã€‚
    è‡ªåŠ¨é€‰æ‹©åç«¯ï¼Œæ ¹æ®è®¾å¤‡ã€‚
    """

    def __init__(
            self,
            model_path: Optional[str],
            device: Optional[str] = None,
            vad_model_path: Optional[str] = None
    ):
        self.model_path = model_path
        self.device = device
        self.data_type = torch.float32  # åˆå§‹åŒ–æ—¶é»˜è®¤ä½¿ç”¨float32

        self.backend = ASRInferToolTorch(model_path, device=device, data_type=self.data_type)

        # VAD åˆå§‹åŒ– - éœ€è¦åœ¨æ¨¡å‹åŠ è½½åè¿›è¡Œ
        self.sample_rate = 16000
        if vad_model_path == "default":
            self.vad_model = load_silero_vad()
        else:
            if not vad_model_path:
                vad_model_path = os.path.join(os.path.dirname(__file__), "models", "vad_model")
                if not os.path.exists(vad_model_path):
                    os.makedirs(vad_model_path, exist_ok=True)
            self.vad_model = torch.package.PackageImporter(vad_model_path).load_pickle("silero_vad", "models")

        # VADæ¨¡å‹åˆå§‹åŒ–æ—¶å…ˆè®¾ç½®ä¸ºCPUï¼Œç­‰é€‰æ‹©è®¾å¤‡åå†ç§»åŠ¨
        self.vad_device = "cpu"  # åˆå§‹è®¾ç½®ä¸ºCPU
        self.vad_model = self.vad_model.to(self.vad_device)

        # å‚æ•°è®¾ç½®
        self.min_speech_duration_ms = 800
        self.max_speech_duration_s = 15.0
        self.continuous_silence_threshold = 1.0
        self.min_voice_duration = 0.5
        self.energy_threshold = 0.01  # è°ƒæ•´èƒ½é‡é˜ˆå€¼

    def load_to_cpu(self):
        """å°†ASRæ¨¡å‹åŠ è½½åˆ°CPU"""
        self.backend.load_to_cpu()
        self.device = "cpu"
        self.data_type = torch.float32
        # åŒæ—¶ç§»åŠ¨VADæ¨¡å‹
        self.vad_model = self.vad_model.to("cpu")
        self.vad_device = "cpu"

    def load_to_gpu(self, device_num: int = 0, if_half: bool = True):
        """å°†ASRæ¨¡å‹åŠ è½½åˆ°GPU"""
        self.backend.load_to_gpu(device_num, if_half)
        device_str = f"cuda:{device_num}" if torch.cuda.is_available() else "cuda"
        self.device = device_str
        self.data_type = torch.float16 if if_half else torch.float32
        # åŒæ—¶ç§»åŠ¨VADæ¨¡å‹
        self.vad_model = self.vad_model.to(device_str)
        self.vad_device = device_str

    def model_load(self):
        # ä¿ç•™åŸæœ‰æ–¹æ³•ï¼Œä½†ç°åœ¨éœ€è¦æŒ‡å®šè®¾å¤‡
        if self.device is None:
            logging.info("æœªé€‰æ‹©åˆå§‹åŒ–è®¾å¤‡ï¼Œé»˜è®¤å°†æ¨¡å‹åŠ è½½åˆ°CPU")
            self.load_to_cpu()
        else:
            if "cuda" in self.device:
                device_num = 0 if ":" not in self.device else int(self.device.split(":")[1])
                self.load_to_gpu(device_num, if_half=(self.data_type == torch.float16))
            else:
                self.load_to_cpu()

    def model_clear(self):
        self.backend.model_clear()
        self.device = None

    def local_file_inference(self, audio_path: str, language: Optional[str] = None, task: str = "transcribe") -> str:
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"Audio file not found: {audio_path}")
        audio, sr = librosa.load(audio_path, sr=16000, mono=True)
        return self.backend.inference(audio, language=language, task=task)

    def _int16_to_float32(self, audio_int16: np.ndarray) -> np.ndarray:
        return audio_int16.astype(np.float32) / 32768.0

    def _is_silence(self, audio_chunk: np.ndarray, threshold: float = 0.01) -> bool:
        """æ£€æŸ¥éŸ³é¢‘å—æ˜¯å¦ä¸ºé™éŸ³ï¼ˆåŸºäºèƒ½é‡ï¼‰"""
        energy = np.sqrt(np.mean(audio_chunk ** 2))
        return energy < threshold

    def microphone_audio_stream(self) -> Generator[np.ndarray, None, None]:
        """
        ç®€å•ä½†å¯é çš„è¯­éŸ³æ´»åŠ¨æ£€æµ‹
        """
        p = pyaudio.PyAudio()
        stream = p.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=800  # 50mså—ï¼Œæé«˜å“åº”é€Ÿåº¦
        )

        # éŸ³é¢‘ç´¯ç§¯ç¼“å†²åŒº
        audio_buffer = np.array([], dtype=np.float32)

        chunk_duration_sec = 0.05  # 50ms
        silence_counter = 0
        voice_detected = False
        max_silence_frames = int(self.continuous_silence_threshold / chunk_duration_sec)  # è½¬æ¢ä¸ºå¸§æ•°

        try:
            while True:
                # è¯»å–éŸ³é¢‘å—
                chunk_bytes = stream.read(int(self.sample_rate * chunk_duration_sec), exception_on_overflow=False)
                chunk_int16 = np.frombuffer(chunk_bytes, dtype=np.int16)
                chunk_float32 = self._int16_to_float32(chunk_int16)

                # èƒ½é‡æ£€æµ‹
                is_silence = self._is_silence(chunk_float32, self.energy_threshold)

                if not is_silence:
                    # æ£€æµ‹åˆ°å£°éŸ³ï¼Œæ·»åŠ åˆ°ç¼“å†²åŒº
                    audio_buffer = np.concatenate([audio_buffer, chunk_float32])
                    silence_counter = 0  # é‡ç½®é™éŸ³è®¡æ•°å™¨
                    voice_detected = True

                    # é™åˆ¶ç¼“å†²åŒºå¤§å°ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
                    max_buffer_size = int(self.sample_rate * self.max_speech_duration_s)
                    if len(audio_buffer) > max_buffer_size:
                        # è¾“å‡ºå½“å‰ç¼“å†²åŒº
                        if len(audio_buffer) > self.sample_rate * self.min_voice_duration:
                            yield audio_buffer.copy()
                        # é‡ç½®
                        audio_buffer = np.array([], dtype=np.float32)
                        voice_detected = False
                else:
                    # æ£€æµ‹åˆ°é™éŸ³
                    if voice_detected:
                        # å¦‚æœä¹‹å‰æ£€æµ‹åˆ°è¯­éŸ³ï¼Œæ·»åŠ é™éŸ³åˆ°ç¼“å†²åŒºï¼ˆç”¨äºè‡ªç„¶è¿‡æ¸¡ï¼‰
                        audio_buffer = np.concatenate([audio_buffer, chunk_float32])
                        silence_counter += 1

                        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é™éŸ³é˜ˆå€¼
                        if silence_counter >= max_silence_frames:
                            # è¾“å‡ºè¯­éŸ³æ®µ
                            if len(audio_buffer) > self.sample_rate * self.min_voice_duration:
                                # ç§»é™¤æœ«å°¾çš„é™éŸ³éƒ¨åˆ†ï¼ˆä¿ç•™æœ€å0.2ç§’é™éŸ³ç”¨äºå¹³æ»‘ï¼‰
                                speech_end_idx = len(audio_buffer) - int(self.sample_rate * 0.2)
                                if speech_end_idx > 0:
                                    audio_buffer = audio_buffer[:speech_end_idx]
                                yield audio_buffer.copy()

                            # é‡ç½®
                            audio_buffer = np.array([], dtype=np.float32)
                            voice_detected = False
                            silence_counter = 0
                    else:
                        # ä¹‹å‰ä¹Ÿæ²¡æœ‰è¯­éŸ³ï¼Œç»§ç»­ç­‰å¾…
                        continue

        except KeyboardInterrupt:
            # ç¨‹åºä¸­æ–­æ—¶è¾“å‡ºå‰©ä½™éŸ³é¢‘
            if len(audio_buffer) > self.sample_rate * self.min_voice_duration:
                yield audio_buffer.copy()
            pass
        finally:
            stream.stop_stream()
            stream.close()
            p.terminate()

    def microphone_inference(self, language: Optional[str] = None, task: str = "transcribe") -> Generator[
        str, None, None]:
        """
        å®æ—¶éº¦å…‹é£è½¬å½•ï¼šé‡‡é›† â†’ VAD â†’ æ¨ç† â†’ yield æ–‡æœ¬
        """
        if not hasattr(self.backend, 'inference'):
            raise NotImplementedError("Backend does not support raw audio transcription.")

        for audio_segment in self.microphone_audio_stream():
            if len(audio_segment) > 0:
                text = self.backend.inference(audio_segment, language=language, task=task)
                if text.strip():
                    yield text.strip()


if __name__ == "__main__":
    config = ConfigManager()
    # åˆå§‹åŒ–æ—¶ä¸è‡ªåŠ¨åŠ è½½æ¨¡å‹
    asr = ASRInference(model_path=config.asr_config.model_path, device=None,
                       vad_model_path=config.asr_config.vad_model_path)

    # æ‰‹åŠ¨é€‰æ‹©åŠ è½½åˆ°GPUæˆ–CPU
    asr.load_to_gpu(device_num=0, if_half=True)  # åŠ è½½åˆ°GPUï¼Œä½¿ç”¨åŠç²¾åº¦
    # asr.load_to_cpu()  # åŠ è½½åˆ°CPU

    for text in asr.microphone_inference(language="zh", task="transcribe"):
        print("ğŸ—£ï¸:", text)
    del asr
    torch.cuda.empty_cache()